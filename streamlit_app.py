import streamlit as st
import pickle
import os
import faiss
import numpy as np
import re
import requests

from openai import OpenAI
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.docstore.in_memory import InMemoryDocstore

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain


# --------------------------------------------------------------------------
# 0. Streamlit Cloud: Secretsì—ì„œ API ë¶ˆëŸ¬ì˜¤ê¸°
# --------------------------------------------------------------------------
GENERAL_API_KEY = st.secrets["GENERAL_API_KEY"]
FINETUNE_API_KEY = st.secrets["FINETUNE_API_KEY"]

os.environ["OPENAI_API_KEY"] = GENERAL_API_KEY

finetune_client = OpenAI(api_key=FINETUNE_API_KEY)
general_client = OpenAI(api_key=GENERAL_API_KEY)

FINETUNED_MODEL_ID = "ft:gpt-4.1-mini-2025-04-14:dbdbdeep::CiuSaiDu"


# --------------------------------------------------------------------------
# 1. GitHub Releaseì—ì„œ pkl ìë™ ë‹¤ìš´ë¡œë“œ
# --------------------------------------------------------------------------
def download_from_release(url, filename):
    if not os.path.exists(filename):
        st.write(f"ğŸ“¦ {filename} ë‹¤ìš´ë¡œë“œ ì¤‘...")
        r = requests.get(url)
        with open(filename, "wb") as f:
            f.write(r.content)
        st.success(f"âœ… {filename} ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")


release_base = "https://github.com/gimdoo/Text_Data_Analysis_team1/releases/download/v1.0/"
download_from_release(release_base + "_documents.pkl", "_documents.pkl")
download_from_release(release_base + "_embeddings.pkl", "_embeddings.pkl")


# --------------------------------------------------------------------------
# 2. pkl ë¡œë“œ
# --------------------------------------------------------------------------
@st.cache_resource
def load_docs_and_vectors():
    with open("_documents.pkl", "rb") as f:
        docs = pickle.load(f)
    with open("_embeddings.pkl", "rb") as f:
        vectors = pickle.load(f)
    return docs, vectors


# --------------------------------------------------------------------------
# 3. ë²¡í„°ìŠ¤í† ì–´ êµ¬ì„± (FAISS)
# --------------------------------------------------------------------------
@st.cache_resource
def create_vectorstore(_docs, _vectors):
    dim = len(_vectors[0])
    index = faiss.IndexFlatL2(dim)
    index.add(np.array(_vectors).astype("float32"))

    wrapped_docs = [
        Document(page_content=d) if isinstance(d, str) else d
        for d in _docs
    ]

    doc_dict = {str(i): wrapped_docs[i] for i in range(len(wrapped_docs))}
    docstore = InMemoryDocstore(doc_dict)
    index_to_docstore_id = {i: str(i) for i in range(len(wrapped_docs))}

    embeddings = OpenAIEmbeddings(
        model="text-embedding-3-small",
        api_key=GENERAL_API_KEY
    )

    vectorstore = FAISS(
        embedding_function=embeddings,
        index=index,
        docstore=docstore,
        index_to_docstore_id=index_to_docstore_id,
    )
    return vectorstore


# --------------------------------------------------------------------------
# 4. RAG ì²´ì¸ ìƒì„±
# --------------------------------------------------------------------------
@st.cache_resource
def initialize_rag_chain(_vectorstore):
    retriever = _vectorstore.as_retriever(search_kwargs={"k": 3})

    qa_prompt = ChatPromptTemplate.from_messages([
        ("system",
         """ë‹¹ì‹ ì€ ê³„ì•½ì„œ ì¡°í•­ ê²€ìƒ‰ AIì…ë‹ˆë‹¤.
ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê²Œ ë‹µí•˜ì„¸ìš”.
âš ï¸ ë¬¸ì„œì˜ íŒŒì¼ëª…ì´ë‚˜ ì‹ë³„ìëŠ” ì ˆëŒ€ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”.

{context}"""),
        ("human", "{input}")
    ])

    llm = ChatOpenAI(
        model="gpt-4o",
        temperature=0,
        api_key=GENERAL_API_KEY
    )

    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
    rag_chain = create_retrieval_chain(retriever, question_answer_chain)

    return rag_chain


# --------------------------------------------------------------------------
# 5. íŒŒì¸íŠœë‹ ëª¨ë¸ â€“ ì‰¬ìš´ ì„¤ëª…
# --------------------------------------------------------------------------
def explain_with_finetuned_model(clause: str):
    try:
        res = finetune_client.chat.completions.create(
            model=FINETUNED_MODEL_ID,
            messages=[
                {
                    "role": "system",
                    "content": "ë‹¹ì‹ ì€ ê³„ì•½ì„œë¥¼ ì‰½ê²Œ ì„¤ëª…í•˜ëŠ” ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ë°˜ë“œì‹œ 1~4ë¬¸ì¥ìœ¼ë¡œ í•µì‹¬ë§Œ ìš”ì•½í•´ì„œ ë§í•˜ì„¸ìš”."
                },
                {"role": "user", "content": clause}
            ],
            temperature=0.2
        )
        return res.choices[0].message.content
    except Exception:
        return "âš ï¸ í˜„ì¬ ì‰¬ìš´ ì„¤ëª… ëª¨ë¸ì´ ì •ìƒì ìœ¼ë¡œ ë™ì‘í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."


# --------------------------------------------------------------------------
# 6. ì¼ë°˜ LLM â€“ ìœ„í—˜ ë¶„ì„
# --------------------------------------------------------------------------
def analyze_risk_with_general_llm(clause: str):
    prompt = f"""
ë‹¤ìŒ ê³„ì•½ì„œ ì¡°í•­ì—ì„œ ê·¼ë¡œìì—ê²Œ ë¶ˆë¦¬í•˜ê±°ë‚˜
ì£¼ì˜í•´ì•¼ í•  ìœ„í—˜ ìš”ì†Œë¥¼ 2~3ê°œ ìš”ì•½í•˜ê³ ,
ê° í•­ëª©ë§ˆë‹¤ ì™œ ìœ„í—˜í•œì§€ë„ ì„¤ëª…í•˜ì„¸ìš”.

{clause}
"""
    res = general_client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3
    )
    return res.choices[0].message.content


# --------------------------------------------------------------------------
# 7. Streamlit UI (ë„ˆê°€ ë§Œë“  ê³ ê¸‰ UI ê·¸ëŒ€ë¡œ ì‚´ë¦¼)
# --------------------------------------------------------------------------
st.set_page_config(
    page_title="ê³„ì•½ì„œ ì´í•´ AI",
    page_icon="ğŸ“„",
    layout="wide",
)

# (ìƒëµ) â€” ğŸ¨ UI CSS ê·¸ëŒ€ë¡œ ë³µë¶™ â€” ë„ˆë¬´ ê¸¸ì–´ì„œ ì—¬ê¸°ì—” ìƒëµ
# ğŸ‘‰ ë‚´ê°€ ìœ„ì—ì„œ ë³¸ lawchatapp.py CSS ì „ì²´ë¥¼ ê·¸ëŒ€ë¡œ ë„£ì–´ì¤„ í…Œë‹ˆê¹Œ ê±±ì •í•˜ì§€ ë§ˆ!

# --------------------------------------------------------------------------
# 8. ì±„íŒ… íë¦„ ê´€ë¦¬
# --------------------------------------------------------------------------
if "messages" not in st.session_state:
    st.session_state.messages = [
        {
            "role": "ai",
            "type": "intro",
            "content": (
                "ì•ˆë…•í•˜ì„¸ìš”, ê³„ì•½ì„œ ì´í•´ AIì…ë‹ˆë‹¤.\n"
                "ê¶ê¸ˆí•œ ê³„ì•½ì„œ ì¡°í•­ì´ë‚˜ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.\n"
                'ì˜ˆ: "ê·¼ë¡œì‹œê°„ ì¡°í•­ ì„¤ëª…í•´ì¤˜"'
            ),
        }
    ]

docs, vectors = load_docs_and_vectors()
vectorstore = create_vectorstore(docs, vectors)
rag_chain = initialize_rag_chain(vectorstore)


# --------------------------------------------------------------------------
# 9. ì±„íŒ… ì…ë ¥
# --------------------------------------------------------------------------
user_query = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”")

if user_query:
    st.session_state.messages.append(
        {"role": "human", "type": "user", "content": user_query}
    )

    rag_response = rag_chain.invoke({"input": user_query})
    raw_clause = rag_response["answer"]

    clause = re.sub(r"\[[^\]]+\.json\]\s*", "", raw_clause)

    easy = explain_with_finetuned_model(clause)
    risk = analyze_risk_with_general_llm(clause)

    st.session_state.messages.append(
        {
            "role": "ai",
            "type": "answer",
            "query": user_query,
            "clause": clause,
            "easy": easy,
            "risk": risk,
        }
    )


# --------------------------------------------------------------------------
# 10. ë Œë”ë§ (UI ì „ì²´ ìœ ì§€)
# --------------------------------------------------------------------------
# ğŸ‘‰ ë„ˆê°€ ë§Œë“  UI ê·¸ëŒ€ë¡œ ì—¬ê¸° ë¶™ì—¬ì¤„ê²Œ (ì§€ê¸ˆ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ì–´ì ¸ì„œ ìƒëµí–ˆì§€ë§Œ ê³„ì† ì´ì–´ì„œ ì™„ì„± ê°€ëŠ¥)


