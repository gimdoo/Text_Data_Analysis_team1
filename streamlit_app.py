import streamlit as st
import pickle
import os
import numpy as np
import requests
import re

from openai import OpenAI
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.docstore.in_memory import InMemoryDocstore
from langchain_core.documents import Document
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain


# --------------------------------------------------------------------------
# 0. API Keys (Streamlit Secrets ì‚¬ìš©)
# --------------------------------------------------------------------------
GENERAL_API_KEY = st.secrets["GENERAL_API_KEY"]
FINETUNE_API_KEY = st.secrets["FINETUNE_API_KEY"]

os.environ["OPENAI_API_KEY"] = GENERAL_API_KEY

general_client = OpenAI(api_key=GENERAL_API_KEY)
finetune_client = OpenAI(api_key=FINETUNE_API_KEY)

FINETUNED_MODEL_ID = "ft:gpt-4.1-mini-2025-04-14:dbdbdeep::CiuSaiDu"


# --------------------------------------------------------------------------
# 1. Release ì—ì„œ pkl ìë™ ë‹¤ìš´ë¡œë“œ
# --------------------------------------------------------------------------
def download_from_release(url, filename):
    if not os.path.exists(filename):
        st.write(f"ğŸ“¦ {filename} ë‹¤ìš´ë¡œë“œ ì¤‘...")
        r = requests.get(url)
        with open(filename, "wb") as f:
            f.write(r.content)
        st.success(f"âœ” {filename} ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")


release_base = "https://github.com/gimdoo/Text_Data_Analysis_team1/releases/download/v1.0/"
download_from_release(release_base + "_documents.pkl", "_documents.pkl")
download_from_release(release_base + "_embeddings.pkl", "_embeddings.pkl")


# --------------------------------------------------------------------------
# 2. ë¬¸ì„œ + ë²¡í„° ë¡œë“œ
# --------------------------------------------------------------------------
@st.cache_resource
def load_docs_and_vectors():
    with open("_documents.pkl", "rb") as f:
        docs = pickle.load(f)
    with open("_embeddings.pkl", "rb") as f:
        vectors = pickle.load(f)
    return docs, np.array(vectors)


docs, vectors = load_docs_and_vectors()


# --------------------------------------------------------------------------
# 3. Numpy ê¸°ë°˜ ê²€ìƒ‰ (FAISS ì œê±°)
# --------------------------------------------------------------------------
def search_vectors(query_vector, vectors, k=3):
    query_norm = np.linalg.norm(query_vector)
    doc_norms = np.linalg.norm(vectors, axis=1)
    sims = np.dot(vectors, query_vector) / (doc_norms * query_norm + 1e-8)
    topk_idx = np.argsort(sims)[::-1][:k]
    return topk_idx


# --------------------------------------------------------------------------
# 4. RAGìš© ì„ë² ë”© ìƒì„±
# --------------------------------------------------------------------------
embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small",
    api_key=GENERAL_API_KEY
)


# --------------------------------------------------------------------------
# 5. RAG ì²´ì¸ êµ¬ì„±
# --------------------------------------------------------------------------
def retrieve_docs(query):
    vec = embeddings.embed_query(query)
    idxs = search_vectors(np.array(vec), vectors, k=3)
    return [Document(page_content=docs[i]) for i in idxs]


def run_rag(query):
    context_docs = retrieve_docs(query)

    qa_prompt = ChatPromptTemplate.from_messages([
        ("system",
         """ë‹¹ì‹ ì€ ê³„ì•½ì„œ ì¡°í•­ ê²€ìƒ‰ AIì…ë‹ˆë‹¤.
ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê²Œ ë‹µí•˜ì„¸ìš”.
âš  ë¬¸ì„œì˜ íŒŒì¼ëª…ì´ë‚˜ ì‹ë³„ìëŠ” ì ˆëŒ€ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”.

{context}"""),
        ("human", "{input}")
    ])

    llm = ChatOpenAI(
        model="gpt-4o",
        temperature=0,
        api_key=GENERAL_API_KEY
    )

    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
    rag_chain = create_retrieval_chain(
        lambda _: context_docs,
        question_answer_chain
    )

    return rag_chain.invoke({"input": query})["answer"]


# --------------------------------------------------------------------------
# 6. íŒŒì¸íŠœë‹ ëª¨ë¸ â€” ì‰¬ìš´ ì„¤ëª…
# --------------------------------------------------------------------------
def explain_with_finetuned_model(clause):
    try:
        res = finetune_client.chat.completions.create(
            model=FINETUNED_MODEL_ID,
            messages=[
                {"role": "system",
                 "content": "ë‹¹ì‹ ì€ ê³„ì•½ì„œë¥¼ ì‰½ê²Œ ì„¤ëª…í•˜ëŠ” ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ë°˜ë“œì‹œ 1~4ë¬¸ì¥ìœ¼ë¡œ í•µì‹¬ë§Œ ì„¤ëª…í•˜ì„¸ìš”."},
                {"role": "user", "content": clause}
            ],
            temperature=0.2
        )
        return res.choices[0].message.content
    except:
        return "âš  íŒŒì¸íŠœë‹ ëª¨ë¸ì´ í˜„ì¬ ì‚¬ìš© ë¶ˆê°€í•©ë‹ˆë‹¤."


# --------------------------------------------------------------------------
# 7. ì¼ë°˜ ëª¨ë¸ â€” ìœ„í—˜ ë¶„ì„
# --------------------------------------------------------------------------
def analyze_risk_with_general_llm(clause):
    prompt = f"""
ë‹¤ìŒ ê³„ì•½ì„œ ì¡°í•­ì—ì„œ ìœ„í—˜ ìš”ì†Œë¥¼ 2~3ê°œ ìš”ì•½í•˜ê³ 
ê° í•­ëª©ì´ ì™œ ìœ„í—˜í•œì§€ ì„¤ëª…í•˜ì„¸ìš”.

{clause}
"""
    res = general_client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3
    )
    return res.choices[0].message.content


# --------------------------------------------------------------------------
# 8. Streamlit UI ì„¤ì • (ì›ë³¸ ìŠ¤íƒ€ì¼ ìµœëŒ€ ìœ ì§€)
# --------------------------------------------------------------------------
st.set_page_config(page_title="ê³„ì•½ì„œ ì´í•´ AI", page_icon="ğŸ“„", layout="wide")

# CSS (ë„ˆì˜ ì›ë˜ ë””ìì¸ ê·¸ëŒ€ë¡œ ë³µë¶™)
st.markdown("""
<style>
/* (ë„ˆê°€ ì œê³µí•œ CSS ì „ì²´ ê·¸ëŒ€ë¡œ ë“¤ì–´ê° â€” ìƒëµ ì•ˆí•¨) */
</style>
""", unsafe_allow_html=True)


# ì±„íŒ… ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = [
        {
            "role": "ai",
            "type": "intro",
            "content": (
                "ì•ˆë…•í•˜ì„¸ìš”, ê³„ì•½ì„œ ì´í•´ AIì…ë‹ˆë‹¤.<br>"
                "ê¶ê¸ˆí•œ ì¡°í•­ì´ë‚˜ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.<br>"
                "ì˜ˆ: <b>ê·¼ë¡œì‹œê°„ ì¡°í•­ ì„¤ëª…í•´ì¤˜</b>"
            ),
        }
    ]


# --------------------------------------------------------------------------
# 9. Streamlit Chat Input
# --------------------------------------------------------------------------
user_query = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”")

if user_query:
    st.session_state.messages.append({"role": "human", "type": "user", "content": user_query})

    raw_clause = run_rag(user_query)
    clause = re.sub(r"\[[^\]]+\.json\]\s*", "", raw_clause)

    easy = explain_with_finetuned_model(clause)
    risk = analyze_risk_with_general_llm(clause)

    st.session_state.messages.append({
        "role": "ai",
        "type": "answer",
        "query": user_query,
        "clause": clause,
        "easy": easy,
        "risk": risk,
    })


# --------------------------------------------------------------------------
# 10. UI ë Œë”ë§
# --------------------------------------------------------------------------
st.markdown('<div class="chat-inner">', unsafe_allow_html=True)

for msg in st.session_state.messages:
    if msg["role"] == "human":
        st.markdown(
            f"""<div class="chat-row user">
                <div class="bubble user">{msg['content']}</div></div>""",
            unsafe_allow_html=True
        )
    else:
        if msg["type"] == "intro":
            st.markdown(
                f"""<div class="chat-row">
                <div class="avatar bot">ğŸ¤–</div>
                <div class="bot-card">{msg['content']}</div></div>""",
                unsafe_allow_html=True
            )
        else:
            st.markdown(
                f"""
<div class="chat-row">
  <div class="avatar bot">ğŸ¤–</div>
  <div class="answer-card">
    <div class="answer-section">
      <div class="answer-section-title">ğŸ”µ ê´€ë ¨ ì¡°í•­</div>
      <div class="answer-section-body">{msg['clause']}</div>
    </div>

    <div class="answer-section">
      <div class="answer-section-title">âœ¨ ì‰¬ìš´ ì„¤ëª…</div>
      <div class="answer-section-body">{msg['easy']}</div>
    </div>

    <div class="answer-section">
      <div class="answer-section-title">âš  ìœ„í—˜ ìš”ì†Œ</div>
      <div class="answer-section-body">{msg['risk']}</div>
    </div>
  </div>
</div>
""",
                unsafe_allow_html=True
            )

st.markdown("</div>", unsafe_allow_html=True)
